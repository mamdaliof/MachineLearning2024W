\documentclass{article}

\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage[margin=1in]{geometry}
\usepackage{listings}
\usepackage[hidelinks]{hyperref}
%\usepackage{subfigure}
\usepackage{subcaption}

\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{float}
\usepackage{booktabs}

\hypersetup{
	colorlinks=true,
	linkcolor=teal,
	filecolor=magenta,      
	urlcolor=teal,
	citecolor = teal
}
\usepackage{xcolor}
\usepackage{xepersian}
\setlength\headheight{28pt} 
\settextfont[
    Path = ./font/, % specify the path to the font files
    Scale = 1.3,
    UprightFont = XB Niloofar, % Regular font
    BoldFont = XB NiloofarBd,       % Bold font
    ItalicFont = XB NiloofarIt,   % Italic font
    BoldItalicFont = XB NiloofarBdIt % Bold Italic font if available
]{XB Niloofar}
\setlatintextfont[Scale=1.3]{Times New Roman}
\renewcommand{\baselinestretch}{1.5}
\pagestyle{fancy}
\fancyhf{}
\rhead{\includegraphics[width=1cm]{img/Logo.png} 
	ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†
	-
	Ù…ÛŒÙ†ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡ Ø´Ù…Ø§Ø±Ù‡ 2}
\lhead{\thepage}
\rfoot{Ø³ÛŒØ¯Ù…Ø­Ù…Ø¯ Ø­Ø³ÛŒÙ†ÛŒ}
\lfoot{9821253}
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{1pt}
\renewcommand{\lstlistingname}{Code}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}

\lstset{style=mystyle}

\begin{document}
	
	\input{titlepage}
	
	\tableofcontents \clearpage
	\listoffigures \clearpage
	\listoftables \clearpage
	\lstlistoflistings \clearpage
	\newpage

\section{Ù¾Ø±Ø³Ø´2}

\subsection{Ù‚Ø³Ù…Øª Ø§ÙˆÙ„}

\lr{lunar lander}
 ÙØ¶Ø§Ù¾ÛŒÙ…Ø§ÛŒÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¨Ø±Ø§ÛŒ ÙØ±ÙˆØ¯ Ø¢Ù…Ø¯Ù† Ø¨Ø± Ø³Ø·Ø­ Ù…Ø§Ù‡ Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª Ùˆ Ù…Ø£Ù…ÙˆØ±ÛŒØª Ø¢Ù† ÙØ±ÙˆØ¯ Ø§ÛŒÙ…Ù† Ø§Ø² Ù…Ø¯Ø§Ø± Ù…Ø§Ù‡ Ø¨Ù‡ Ø³Ø·Ø­ Ù…Ø§Ù‡ Ø§Ø³Øª. Ù‡Ø¯Ù Ø§ØµÙ„ÛŒ Ø§ÛŒÙ† ÙØ¶Ø§Ù¾ÛŒÙ…Ø§ Ø´Ø§Ù…Ù„ Ø·ÛŒ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± Ø¯Ù‚ÛŒÙ‚ ÙØ±ÙˆØ¯ Ùˆ ÙØ±ÙˆØ¯ Ø§ÛŒÙ…Ù† Ùˆ Ù†Ø±Ù… Ø§Ø³Øª Ú©Ù‡ Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ØŒ Ù†Ø§ÙˆØ¨Ø±ÛŒ Ø¯Ù‚ÛŒÙ‚ Ùˆ Ø·Ø±Ø§Ø­ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ Ù‚ÙˆÛŒ Ø§Ø³Øª. Ø³ÛŒØ³ØªÙ… Ù¾ÛŒØ´Ø±Ø§Ù†Ù‡ Ú©Ù‡ Ø´Ø§Ù…Ù„ Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ ÙØ±ÙˆØ¯ Ùˆ Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ Ú©Ù†ØªØ±Ù„ ÙˆØ¶Ø¹ÛŒØª Ø¬Ø§Ù†Ø¨ÛŒ Ùˆ Ú©ÙˆÚ†Ú©ØªØ± Ø§Ø³Øª Ú©Ù‡ Ù†Ù‚Ø´ Ù…Ù‡Ù…ÛŒ Ø¯Ø± Ú©Ù†ØªØ±Ù„ Ø³Ø±Ø¹Øª ÙØ±ÙˆØ¯ Ùˆ Ø¬Ù‡Øªâ€ŒÚ¯ÛŒØ±ÛŒ Ø§ÛŒÙØ§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. \lr{lunar lander} Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¯Ø§Ø±Ø§ÛŒ Ù¾Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ ÙØ±ÙˆØ¯ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ø§Ø«Ø± Ø¶Ø±Ø¨Ù‡ Ùˆ ØªØ¶Ù…ÛŒÙ† Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ Ù‡Ø³ØªÙ†Ø¯. 
 
 ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªÙ‚ÙˆÛŒØªÛŒ (RL) Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ø±Ø§ÛŒ Ù…Ø³Ø¦Ù„Ù‡ Ú©Ù†ØªØ±Ù„ ÛŒÚ©
 \lr{lunar lander}
Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆØ¯. Ø¯Ø± Ø§ÛŒÙ† Ø²Ù…ÛŒÙ†Ù‡ØŒ RL Ø´Ø§Ù…Ù„ Ø¢Ù…ÙˆØ²Ø´ ÛŒÚ© Agent Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ÛŒÚ© Policy Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ ÙØ±ÙˆØ¯ Ø§ÛŒÙ…Ù† Ùˆ Ú©Ø§Ø±Ø¢Ù…Ø¯ Ù…
\lr{lunar lander}
 Ø§Ø³Øª. Ø§Ø¬Ø²Ø§ÛŒ Ø§ØµÙ„ÛŒ ÛŒÚ© Ù…Ø³Ø¦Ù„Ù‡ RL Ø´Ø§Ù…Ù„ ÙØ¶Ø§ÛŒ Ø­Ø§Ù„ØªØŒ ÙØ¶Ø§ÛŒ Ø¹Ù…Ù„ Ùˆ Ø³ÛŒØ³ØªÙ… Ù¾Ø§Ø¯Ø§Ø´ Ø§Ø³Øª Ú©Ù‡ Ø§ÛŒÙ† Ù…ÙˆØ§Ø±Ø¯ Ø¨Ù‡ Ø´Ø±Ø­ Ø²ÛŒØ± Ù…ÛŒâ€ŒØ¨Ø§Ø´Ù†Ø¯.
 
 \textbf{ÙØ¶Ø§ÛŒ Ø­Ø§Ù„Øª}
 
 ÙØ¶Ø§ÛŒ Ø­Ø§Ù„Øª ØªÙ…Ø§Ù… Ø­Ø§Ù„Ø§Øª Ù…Ù…Ú©Ù† Ø±Ø§ Ú©Ù‡ ÙØ±ÙˆØ¯Ú¯Ø± Ù‚Ù…Ø±ÛŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¯Ø± Ø¢Ù†â€ŒÙ‡Ø§ Ø¨Ø§Ø´Ø¯ØŒ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. Ø¨Ø±Ø§ÛŒ ÛŒÚ© \lr{lunar lander} ÙØ¶Ø§ÛŒ Ø­Ø§Ù„Øª Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø´Ø§Ù…Ù„ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± Ø¨Ø§Ø´Ø¯:
  \begin{itemize}
 \item Ù…Ø®ØªØµØ§Øª Ø¹Ù…ÙˆØ¯ÛŒ Ùˆ Ø§ÙÙ‚ÛŒ ÙØ±ÙˆØ¯Ú¯Ø±.
 \item Ù…ÙˆÙ„ÙÙ‡â€ŒÙ‡Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¯Ø± Ø¬Ù‡Øªâ€ŒÙ‡Ø§ÛŒ Ø§ÙÙ‚ÛŒ Ùˆ Ø¹Ù…ÙˆØ¯ÛŒ.
 \item Ø²Ø§ÙˆÛŒÙ‡ ÙØ±ÙˆØ¯Ú¯Ø± Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø¹Ù…ÙˆØ¯.
 \item Ø³Ø±Ø¹Øª Ø²Ø§ÙˆÛŒÙ‡â€ŒØ§ÛŒ ÙØ±ÙˆØ¯Ú¯Ø±.
 \item ÙˆØ¶Ø¹ÛŒØª Ù¾Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ ÙØ±ÙˆØ¯Ú¯Ø±
  \end{itemize}
 Ù‡Ø± Ø­Ø§Ù„Øª Ù†Ù…Ø§ÛŒØ§Ù†Ú¯Ø± Ø´Ø±Ø§ÛŒØ· ÙØ±ÙˆØ¯Ú¯Ø± Ø¯Ø± ÛŒÚ© Ú¯Ø§Ù… Ø²Ù…Ø§Ù†ÛŒ Ù…Ø¹ÛŒÙ† Ø§Ø³Øª.
 
 \textbf{ÙØ¶Ø§ÛŒ Ø¹Ù…Ù„}
 
 ÙØ¶Ø§ÛŒ Ø¹Ù…Ù„ ØªÙ…Ø§Ù… Actions Ù…Ù…Ú©Ù† Ø±Ø§ Ú©Ù‡ Agent Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡Ø¯ØŒ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. Ø¨Ø±Ø§ÛŒ ÛŒÚ© \lr{lunar lander} ÙØ¶Ø§ÛŒ Ø¹Ù…Ù„ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø´Ø§Ù…Ù„ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± Ø¨Ø§Ø´Ø¯:
\begin{itemize}
 \item ØªÙˆØ§Ù† Ù…ÙˆØªÙˆØ± Ø§ØµÙ„ÛŒ.
 \item ØªÙˆØ§Ù† Ù…ÙˆØªÙˆØ± Ú†Ù¾.
 \item ØªÙˆØ§Ù† Ù…ÙˆØªÙˆØ± Ø±Ø§Ø³Øª.
 \item Ø§Ù†Ø¬Ø§Ù… Ù†Ø¯Ø§Ø¯Ù† Ù‡ÛŒÚ† Ú©Ø§Ø±ÛŒ.
\end{itemize}

 Ø§ÛŒÙ† Actions Ù†ÛŒØ±ÙˆÛŒ Ù¾ÛŒØ´Ø±Ø§Ù†Ù‡ Ùˆ Ø¬Ù‡Øªâ€ŒÚ¯ÛŒØ±ÛŒ ÙØ±ÙˆØ¯Ú¯Ø± Ø±Ø§ Ú©Ù†ØªØ±Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯.
 
 \textbf{Ø³ÛŒØ³ØªÙ… Ù¾Ø§Ø¯Ø§Ø´}
 
 Ø³ÛŒØ³ØªÙ… Ù¾Ø§Ø¯Ø§Ø´ Ø¨Ø§Ø²Ø®ÙˆØ±Ø¯ÛŒ Ø¨Ù‡ Agent Ø¨Ø± Ø§Ø³Ø§Ø³ Actions Ø¢Ù† Ùˆ ØªØºÛŒÛŒØ±Ø§Øª Ø­Ø§Ù„Øª Ù†Ø§Ø´ÛŒ Ø§Ø² Ø¢Ù†â€ŒÙ‡Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. Ø¨Ø±Ø§ÛŒ ÛŒÚ© \lr{lunar lander} Ø³ÛŒØ³ØªÙ… Ù¾Ø§Ø¯Ø§Ø´ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨Ù‡ Ø§ÛŒÙ† ØµÙˆØ±Øª Ø·Ø±Ø§Ø­ÛŒ Ø´ÙˆØ¯:
 \begin{itemize}

 \item Ù¾Ø§Ø¯Ø§Ø´ Ù…Ø«Ø¨Øª Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªÛŒØ§Ø¨ÛŒ Ø¨Ù‡ ÙØ±ÙˆØ¯ Ø§ÛŒÙ…Ù† Ø¨Ø§ Ø³Ø±Ø¹Øª Ú©Ù….
 \item Ù¾Ø§Ø¯Ø§Ø´ Ù…Ù†ÙÛŒ Ø¨Ø±Ø§ÛŒ ÙØ±ÙˆØ¯Ù‡Ø§ÛŒ Ø³Ø®Øª ÛŒØ§ Ø³Ù‚ÙˆØ·â€ŒÙ‡Ø§.
 \item Ù¾Ø§Ø¯Ø§Ø´ Ù…Ù†ÙÛŒ Ø¨Ø±Ø§ÛŒ Ù…ØµØ±Ù Ø³ÙˆØ®Øª Ø²ÛŒØ§Ø¯.
 \item Ù¾Ø§Ø¯Ø§Ø´ Ù…Ø«Ø¨Øª Ø¨Ø±Ø§ÛŒ Ø­ÙØ¸ Ø¬Ù‡Øªâ€ŒÚ¯ÛŒØ±ÛŒ Ù¾Ø§ÛŒØ¯Ø§Ø±.
 \item Ù¾Ø§Ø¯Ø§Ø´ Ù…Ù†ÙÛŒ Ú©ÙˆÚ†Ú© Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú¯Ø§Ù… Ø²Ù…Ø§Ù†ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ´ÙˆÛŒÙ‚ ÙØ±ÙˆØ¯ Ø³Ø±ÛŒØ¹â€ŒØªØ±.
   \end{itemize}
 Ø³ÛŒØ³ØªÙ… Ù¾Ø§Ø¯Ø§Ø´ Agent Ø±Ø§ ØªØ´ÙˆÛŒÙ‚ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ Policyâ€ŒÙ‡Ø§ÛŒÛŒ Ø±Ø§ Ø¨ÛŒØ§Ù…ÙˆØ²Ø¯ Ú©Ù‡ Ù…Ù†Ø¬Ø± Ø¨Ù‡ ÙØ±ÙˆØ¯Ù‡Ø§ÛŒ Ø§ÛŒÙ…Ù†ØŒ Ú©Ø§Ø±Ø¢Ù…Ø¯ Ùˆ Ù¾Ø§ÛŒØ¯Ø§Ø± Ø´ÙˆØ¯.
 \cite{xu_2024}

\subsection{Ù‚Ø³Ù…Øª Ø¯ÙˆÙ…}

Ø§Ø¨ØªØ¯Ø§ Ø¨Ù‡ Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ù„Ø§Ú©â€ŒÙ‡Ø§ÛŒ Ú©Ø¯ Ù…ÛŒâ€ŒÙ¾Ø±Ø¯Ø§Ø²ÛŒÙ….
\begin{LTR}
	\begin{lstlisting}[language=Python, caption=Deep Neural Network]
# DQN
import torch.nn as nn
import torch.nn.functional as F

class DeepQNetwork(nn.Module):
    def __init__(self, state_size, action_size) -> None:
        super(DeepQNetwork, self).__init__()
        # TODO: define the architecture
        # NOTE: input=observation/state, output=action
        net_list = nn.ModuleList([
            torch.nn.Linear(state_size, 512),
            torch.nn.ReLU(),
            torch.nn.LayerNorm(512),
            torch.nn.Dropout(0.1),
            torch.nn.Linear(512, 512),
            torch.nn.ReLU(),
            torch.nn.LayerNorm(512),
            torch.nn.Dropout(0.1),
            torch.nn.Linear(512, 512),
            torch.nn.ReLU(),
            torch.nn.Linear(512, action_size)
        ])
        self.net = torch.nn.Sequential(*net_list).to(device)

    def forward(self, x):
        # TODO: forward propagation
        # NOTE: use ReLu for activation function in all layers
        # NOTE: last layer has no activation function (predict action)
        # ReLU is created in init, no need here
        x.to(device)
        x = self.net(x)
        return x
	\end{lstlisting}
\end{LTR}
Ú©Ø¯ ÙÙˆÙ‚ ÛŒÚ© Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ø¹Ù…ÛŒÙ‚ Ø±Ø§ Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¨Ù‡ Ù…ÙˆØ¬Ø¨ Ø¢Ù† 4 Ù„Ø§ÛŒÙ‡ Ø®Ø·ÛŒ Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ 3 Ù„Ø§ÛŒÙ‡ ØºÛŒØ± Ø®Ø·ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù‡ Ø§Ø³Øª ØªØ§ Ù…Ø§ØªØ±ÛŒØ³ Ù¾Ø§Ø¯Ø§Ø´ Ø±Ø§ Ù¾ÛŒØ´Ø¨ÛŒÙ†ÛŒ Ú©Ù†Ø¯.


\begin{LTR}
	\begin{lstlisting}[language=Python, caption=Agent Class]
# DQN agent
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

class DQNAgent():
    # NOTE: DON'T change initial values
    def __init__(self, state_size, action_size, batch_size,
                 gamma=0.99, buffer_size=25000, alpha=1e-4):
        # network parameter
        self.state_size = state_size
        self.action_size = action_size

        # hyperparameters
        self.batch_size = batch_size
        self.gamma = gamma

        # experience replay
        self.experience_replay = ExperienceReplay(buffer_size)

        # network
        self.value_net = DeepQNetwork(state_size, action_size).to(device)

        # optimizer
        # TODO: create adam for optimizing network's parameter (learning rate=alpha)
        self.optimizer = optim.Adam(self.value_net.parameters(), lr=alpha)

    def take_action(self, state, eps=0.0):
        # TODO: take action using e-greedy policy
        # NOTE: takes action using the greedy policy with a probability of 1âˆ’ðœ– and a random action with a probability of ðœ–
        # NOTE:
        self.value_net.eval()
        if len(state) != 8 :
            state = state[0]
        rand_eps = random.random()
        if rand_eps > eps:
            with torch.no_grad():
                # print(state)
                return torch.argmax(self.value_net(torch.tensor(state).to(device))).detach().cpu().numpy()
        else:
            return np.random.randint(0, self.action_size)

    def update_params(self):
        if len(self.experience_replay) < self.batch_size:
            return
        # transition batch
        batch = Transition(*zip(*self.experience_replay.sample(self.batch_size)))

        temp = []
        for indx in range(len(batch.state)):
            if len(batch.state[indx]) != 8:
                temp.append(batch.state[indx][0])
            else:
                temp.append(batch.state[indx])
        state_batch = torch.from_numpy(np.vstack(temp)).float().to(device)  # [8, 8]
        action_batch = torch.tensor(np.vstack(batch.action)).long().to(device) # [8, 1]
        next_state_batch = torch.from_numpy(np.vstack(batch.next_state)).float().to(device) # [8, 8]
        reward_batch = torch.tensor(np.vstack(batch.reward)).float().to(device) # [8, 1]
        done_batch = torch.tensor(np.vstack(batch.done)).to(device)

        # calculate loss w.r.t DQN algorithm
        self.value_net.train()
        # STEP1
        q_expected = self.value_net(state_batch).gather(1, action_batch)
        # TODO: compute the expected Q values [y]
        # STEP2
        # TODO: compute Q values [Q(s_t, a)]
        q_targets_next = self.value_net(next_state_batch).detach().max(1)[0].unsqueeze(1)
        q_targets = reward_batch + (self.gamma * q_targets_next * (1 - done_batch*1))
        # STEP3
        # TODO: compute mse loss
        loss = nn.functional.mse_loss(q_expected, q_targets)
        # TODO: optimize the model
        # NOTE: DON'T forget to set the gradients to zeros
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    def save(self, fname):
        # TODO: save checkpoint
        torch.save(self.value_net, fname)

    def load(self, fname, device):
        # TODO: load checkpoint
        self.value_net = torch.load(fname, device)
	\end{lstlisting}
\end{LTR}

Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ØŒ Ú©Ù„Ø§Ø³ \texttt{DQNAgent} ØªØ¹Ø±ÛŒÙ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Agent Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ DQN Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ Ù…Ø³Ø¦ÙˆÙ„ Ù…Ø¯ÛŒØ±ÛŒØª Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒØŒ Ø§Ø¬Ø±Ø§ÛŒ  
\lr{Policy $\epsilon$-Greedy}
ØŒ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø´Ø¨Ú©Ù‡ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ùˆ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø§Ø³Øª.

\subsubsection{ØªØ¹Ø±ÛŒÙ Ù…ØªØºÛŒØ±Ù‡Ø§ Ùˆ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø§ÙˆÙ„ÛŒÙ‡}

Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ÛŒ Ú©Ù„Ø§Ø³ØŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø§ÙˆÙ„ÛŒÙ‡ Ùˆ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø´Ø¨Ú©Ù‡ ØªØ¹Ø±ÛŒÙ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯. Ø§ÛŒÙ† Ø´Ø§Ù…Ù„ Ø§Ù†Ø¯Ø§Ø²Ù‡ State
 (\texttt{state\_size})ØŒ Ø§Ù†Ø¯Ø§Ø²Ù‡ Action
 (\texttt{action\_size})ØŒ Ø§Ù†Ø¯Ø§Ø²Ù‡ 
 Batch
  (\texttt{batch\_size})ØŒ (\texttt{gamma})ØŒ Ø§Ù†Ø¯Ø§Ø²Ù‡ Buffer  (\texttt{buffer\_size}) Ùˆ Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ (\texttt{alpha}) Ø§Ø³Øª.



Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡
Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ (\texttt{DeepQNetwork}) Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Q Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø² \texttt{Adam} Ø¨Ø§ Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ $\alpha$ ØªØ¹Ø±ÛŒÙ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.



\subsubsection{Ù…ØªØ¯ \texttt{take\_action}}

Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø§Ù‚Ø¯Ø§Ù… Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø²
 \lr{Policy $\epsilon$-Greedy} 
 Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø¨Ø§ Ø§Ø­ØªÙ…Ø§Ù„ $1 - \epsilon$ØŒ Ø§Ù‚Ø¯Ø§Ù… Ø¨Ù‡ÛŒÙ†Ù‡ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø¨Ø§ Ø§Ø­ØªÙ…Ø§Ù„ $\epsilon$ØŒ ÛŒÚ© Ø§Ù‚Ø¯Ø§Ù… ØªØµØ§Ø¯ÙÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯.


\subsubsection{Ù…ØªØ¯ \texttt{update\_params}}

Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ø±Ø§ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§Ú¯Ø± ØªØ¹Ø¯Ø§Ø¯ ØªØ¬Ø±Ø¨ÛŒØ§Øª Ú©Ù…ØªØ± Ø§Ø² \texttt{batch\_size} Ø¨Ø§Ø´Ø¯ØŒ ØªØ§Ø¨Ø¹ Ù…ØªÙˆÙ‚Ù Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ ØªØ¬Ø±Ø¨ÛŒØ§Øª Ù†Ù…ÙˆÙ†Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ø´Ø¯Ù‡ Ùˆ Ø¨Ù‡ ÙØ±Ù…Øª Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.\\
Ø­Ø§Ù„ Ù…Ù‚Ø§Ø¯ÛŒØ± Q Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø± (\texttt{q\_expected}) Ùˆ Ø³Ù¾Ø³ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù‡Ø¯Ù Q (\texttt{q\_targets}) Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯. Ø³Ù¾Ø³ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² (MSE) ØªØ§Ø¨Ø¹ Ù‡Ø²ÛŒÙ†Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Ù‡ Ùˆ Ù…Ø¯Ù„ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯.




\begin{LTR}
	\begin{lstlisting}[language=Python, caption=Train]
# training phase

# TODO: create agent
agent = DQNAgent(state_size, action_size, batch_size=BATCH_SIZE)

crs = np.zeros(n_episodes) # cummulative rewards
crs_recent = deque(maxlen=25) # recent cummulative rewards

# training loop
for i_episode in range(1, n_episodes+1):
    # TODO: initialize the environment and state
    if i_episode % 50 == 0:
        env = RecordVideo(gym.make("LunarLander-v2"), f"./DQN/batch{BATCH_SIZE}/eps{i_episode}")
    else:
        env = gym.make("LunarLander-v2")
    state = env.reset()
    done = False
    cr = 0 # episode cummulative rewards
    while not done:
        env.render()
        # TODO: select and perform an action
        action = agent.take_action(state, eps)
#         print(action)
        # Modify the unpacking to handle the extra value if present
        result = env.step(action)
        if len(result) == 5:
            next_state, reward, done, truncated, info = result
        else:
            next_state, reward, done, info = result
        # TODO: store transition in experience replay
        agent.experience_replay.store_trans(state, action, next_state, reward, done)
        # TODO: update agent
        agent.update_params()
        # TODO: update current state and episode cummulative rewards
        # print("next" ,next_state)
        state = next_state
        cr += reward

    # TODO: decay epsilon
    eps = eps * eps_decay_rate
    eps = max(eps, eps_end)
    # TODO: update current cummulative rewards and recent cummulative rewards
    crs[i_episode - 1] = cr
    crs_recent.append(cr)
    # TODO: save agent every 50 episodes
    if i_episode % 50 == 0:
        agent.save(f"q_net_batch{BATCH_SIZE}_eps{i_episode}.pt")

    # print logs
    print('\rEpisode {}\tAverage Reward: {:.2f}\tEpsilon: {:.2f}'.format(i_episode, np.mean(crs_recent), eps), end="")
    if i_episode % 25 == 0:
        print('\rEpisode {}\tAverage Reward: {:.2f}\tEpsilon: {:.2f}'.format(i_episode, np.mean(crs_recent), eps))
	\end{lstlisting}
\end{LTR}



\subsubsection{Training}

Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ØŒ Ú©Ø¯Ù‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¨Ø®Ø´ Training ØªÙˆØ¶ÛŒØ­ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø§ÛŒÙ† ÙØ§Ø² Ø´Ø§Ù…Ù„ Ø§ÛŒØ¬Ø§Ø¯ AgentØŒ Ø§Ø¬Ø±Ø§ÛŒ Ø­Ù„Ù‚Ù‡ Ø¢Ù…ÙˆØ²Ø´ØŒ Ø§Ù†ØªØ®Ø§Ø¨ ActionsØŒ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ØŒ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ø§Ø³Øª.


Ø¯Ø± Ú¯Ø§Ù… Ù†Ø®Ø³Øª Agent Ù…Ø¯ Ù†Ø¸Ø± Ù…Ø§ Ø¨Ø§ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø§ÙˆÙ„ÛŒÙ‡ Ù…ØªÙ†Ø§Ø³Ø¨ Ø¨Ø§ Ù…Ø³Ø¦Ù„Ù‡ Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ 
Ù…ØªØºÛŒØ± \texttt{crs} Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù¾Ø§Ø¯Ø§Ø´â€ŒÙ‡Ø§ÛŒ ØªØ¬Ù…Ø¹ÛŒ Ù‡Ø± Ø§Ù¾ÛŒØ²ÙˆØ¯ Ùˆ \texttt{crs\_recent} Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù¾Ø§Ø¯Ø§Ø´â€ŒÙ‡Ø§ÛŒ ØªØ¬Ù…Ø¹ÛŒ Ø§Ø®ÛŒØ± ØªØ¹Ø±ÛŒÙ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\\
Ø­Ù„Ù‚Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø±Ø§ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ø§Ù¾ÛŒØ²ÙˆØ¯Ù‡Ø§ÛŒ Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡ (\texttt{n\_episodes}) Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯ ØªØ§ Agent Ù¾Ø§Ø³Ø® Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ù‡ Ø³Ù…Ø¦Ù„Ù‡ Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ù†Ø¯.
Ø¯Ø± Ø§ÛŒÙ† Ø­Ù„Ù‚Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ù‡ Ø§Ø²Ø§ÛŒ Ù‡Ø± Ø§Ù¾ÛŒØ²ÙˆØ¯ØŒ Ù…Ø­ÛŒØ· Ùˆ Ø­Ø§Ù„Øª Ø§ÙˆÙ„ÛŒÙ‡ Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯. Ù‡Ø± 50 Ø§Ù¾ÛŒØ²ÙˆØ¯ØŒ Ù…Ø­ÛŒØ· Ø¨Ø±Ø§ÛŒ  RecordVideo ØªÙ†Ø¸ÛŒÙ… Ù…ÛŒâ€ŒØ´ÙˆØ¯.

ØªØ§ Ø²Ù…Ø§Ù†ÛŒ Ú©Ù‡ Ø§Ù¾ÛŒØ²ÙˆØ¯ Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ù†Ø±Ø³ÛŒØ¯Ù‡ Ø§Ø³ØªØŒ Ø­Ù„Ù‚Ù‡ steps Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø¯Ø± Ù‡Ø± Ú¯Ø§Ù…ØŒ Ù…Ø­ÛŒØ· Ø±Ù†Ø¯Ø± Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Agent ÛŒÚ© Ø§Ù‚Ø¯Ø§Ù… Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
Ù†ØªÛŒØ¬Ù‡ Ø§Ù‚Ø¯Ø§Ù… Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡ Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø§Ù†ØªÙ‚Ø§Ù„ Ø¯Ø± ØªØ¬Ø±Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ Agent Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Agent Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ Ùˆ Ø­Ø§Ù„Øª ÙØ¹Ù„ÛŒ Ùˆ Ù¾Ø§Ø¯Ø§Ø´â€ŒÙ‡Ø§ÛŒ ØªØ¬Ù…Ø¹ÛŒ Ø§Ù¾ÛŒØ²ÙˆØ¯ Ø¨Ù‡â€ŒØ±ÙˆØ² Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.
Ù¾Ø³ Ø§Ø² Ù¾Ø§ÛŒØ§Ù† Ø§Ù¾ÛŒØ²ÙˆØ¯ØŒ Ù…Ù‚Ø¯Ø§Ø± \texttt{epsilon} Ú©Ø§Ù‡Ø´ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯ ØªØ§ Ø¨Ù‡ ØªØ¯Ø±ÛŒØ¬ Agent Ú©Ù…ØªØ± Ø¨Ù‡ Actions ØªØµØ§Ø¯ÙÛŒ Ø±ÙˆÛŒ Ø¨ÛŒØ§ÙˆØ±Ø¯.
Ù¾Ø§Ø¯Ø§Ø´â€ŒÙ‡Ø§ÛŒ ØªØ¬Ù…Ø¹ÛŒ Ø§Ù¾ÛŒØ²ÙˆØ¯ Ø¬Ø§Ø±ÛŒ Ùˆ Ù¾Ø§Ø¯Ø§Ø´â€ŒÙ‡Ø§ÛŒ ØªØ¬Ù…Ø¹ÛŒ Ø§Ø®ÛŒØ± Ø¨Ù‡â€ŒØ±ÙˆØ² Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.
Ù‡Ø± 50 Ø§Ù¾ÛŒØ²ÙˆØ¯ØŒ Ù…Ø¯Ù„ Agent Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.


\subsubsection{Ù†ØªØ§ÛŒØ¬}
Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø²ÛŒØ± Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬ Ø­Ø§ØµÙ„Ù‡ Ø§Ø² Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø±Ø³Ù… Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.
\begin{figure}[H]
\centering
\includegraphics[width=1.0\linewidth]{img/result1}
\caption{Ù†ØªØ§ÛŒØ¬ Ø¨Ù‡ Ø§Ø²Ø§ÛŒ ØªÙ…Ø§Ù… Ø§Ù¾ÛŒØ²ÙˆØ¯Ù‡Ø§}
\label{fig:result1}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=1.0\linewidth]{img/result2}
\caption{Ù†ØªØ§ÛŒØ¬ Ø¨Ù‡ Ø§Ø²Ø§ÛŒ Ù‡Ø± 50 Ø§Ù¾ÛŒØ²ÙˆØ¯}
\label{fig:result2}
\end{figure}
Ù‡Ù…Ø§Ù†Ø·ÙˆØ± Ú©Ù‡ Ø§Ù†ØªØ¸Ø§Ø± Ù…ÛŒâ€ŒØ±ÙØª Ù†ØªØ§Ø¬ Ø¨Ù‡ Ø§Ø²Ø§ÛŒ Ø¨Ú† 128 Ø²ÙˆØ¯ØªØ± Ù‡Ù…Ú¯Ø±Ø§ Ø´Ø¯Ù‡ Ø§Ø³Øª Ø§Ù…Ø§ Ø¨Ù‡ Ø§Ø²Ø§ÛŒ Ø¨Ú† 64 Ù†ØªØ§ÛŒØ¬ Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒ Ø¶Ø¹ÛŒÙØªØ±ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³Øª Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø­Ø§Ù„ØªÛŒ Ú©Ù‡ Ø¨Ú† 32 Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª. Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ† Ø¨Ù‡ØªØ±ÛŒÙ† Ø­Ø§Ù„Øª Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¨Ú†128 Ø¨ÙˆØ¯Ù‡ Ùˆ Ø¨Ù‡ Ù…Ù†Ø¸ÙˆØ± ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø³Ø±Ø¹Øª Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒ Ø¯Ø± Ø§Ù¾ÛŒØ²ÙˆØ¯ 50 Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯.

Ú©Ø¯ Ø§Ø¬Ø±Ø§ÛŒÛŒ Ø¨Ù‡ Ù…Ù†Ø¸ÙˆØ± Ø°Ø®ÛŒØ±Ù‡ ÙˆÛŒØ¯ÛŒÙˆ Ø§Ø² Ø¢Ù…ÙˆØ²Ø´ Agent Ù‡Ù…Ø±Ø§Ù‡ Ø¨Ø§ ÛŒÚ© Ø§Ø±ÙˆØ± Ø¨ÙˆØ¯Ù‡ Ú©Ù‡ Ø§Ù…Ú©Ø§Ù† Ø­Ù„ Ø¢Ù† ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´Øª. Ø§Ø±ÙˆØ± Ø¯Ø± ÙØ§ÛŒÙ„ Ù†ÙˆØªØ¨ÙˆÚ© Ù…ÙˆØ¬ÙˆØ¯ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯.

\subsection{Ù‚Ø³Ù…Øª Ø³ÙˆÙ…}


Ø¯Ø± ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªÙ‚ÙˆÛŒØªÛŒØŒ DQN Ùˆ DDQN Ø¯Ùˆ Ø±ÙˆØ´ Ù…Ø­Ø¨ÙˆØ¨ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Q-Ù…Ù‚Ø§Ø¯ÛŒØ± Ù‡Ø³ØªÙ†Ø¯. Ù‡Ø± Ø¯Ùˆ Ø±ÙˆØ´ Ø¨Ù‡ Ù…Ù†Ø¸ÙˆØ± Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Agent Ø¯Ø± Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Actions Ø¯Ø± ÛŒÚ© Ù…Ø­ÛŒØ· Ù…Ø´Ø®Øµ ØªÙˆØ³Ø¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯. Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ ØªÙØ§ÙˆØªâ€ŒÙ‡Ø§ Ùˆ Ø´Ø¨Ø§Ù‡Øªâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ Ø¨ÛŒÙ† DQN Ùˆ DDQN Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….

\subsubsection{DQN}

\begin{itemize}
  \item \textbf{Ù…Ø¹Ù…Ø§Ø±ÛŒ:} DQN Ø§Ø² ÛŒÚ© Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ø¹Ù…ÛŒÙ‚ Ø¨Ø±Ø§ÛŒ ØªÙ‚Ø±ÛŒØ¨ Q-Ù…Ù‚Ø§Ø¯ÛŒØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§ÛŒÙ† Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ Ø´Ø§Ù…Ù„ Ø­Ø§Ù„Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø­ÛŒØ· Ø±Ø§ Ø¯Ø±ÛŒØ§ÙØª Ú©Ø±Ø¯Ù‡ Ùˆ Q-Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù‡Ø± Ø§Ù‚Ø¯Ø§Ù… Ù…Ù…Ú©Ù† Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
  \item \textbf{Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Q-Ù…Ù‚Ø§Ø¯ÛŒØ±:} Ø¯Ø± DQNØŒ Ø§Ø² ÛŒÚ© Ø´Ø¨Ú©Ù‡ Ù‡Ø¯Ù  Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ú©Ù¾ÛŒâ€ŒØ§ÛŒ Ø§Ø² Ø´Ø¨Ú©Ù‡ Ø§ØµÙ„ÛŒ (Q-Network) Ø§Ø³Øª Ùˆ Ø¯Ø± ÙÙˆØ§ØµÙ„ Ø²Ù…Ø§Ù†ÛŒ Ù…Ù†Ø¸Ù… Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø§ÛŒÙ† Ú©Ø§Ø± Ø¨Ù‡ Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
  \item \textbf{Ù…Ø¹Ø§Ø¯Ù„Ù‡ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ:} Ù…Ø¹Ø§Ø¯Ù„Ù‡ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Q-Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¯Ø± DQN Ø¨Ù‡ ØµÙˆØ±Øª Ø²ÛŒØ± Ø§Ø³Øª:
  \[
  Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q'(s', a') - Q(s, a) \right)
  \]
  Ú©Ù‡ Ø¯Ø± Ø¢Ù† \( Q'(s', a') \) Ù…Ù‚Ø¯Ø§Ø± Q Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø´Ø¯Ù‡ ØªÙˆØ³Ø· Ø´Ø¨Ú©Ù‡ Ù‡Ø¯Ù Ø§Ø³Øª.
  \item \textbf{\lr{Over fitting}} Ø¯Ø± DQNØŒ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² \(\max_{a'} Q(s', a')\) Ø¨Ø±Ø§ÛŒ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Q-Ù…Ù‚Ø§Ø¯ÛŒØ±ØŒ Ø§Ø­ØªÙ…Ø§Ù„ \lr{Over fitting}ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù…Ù†Ø¬Ø± Ø¨Ù‡ Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ Ú©Ù…ØªØ± Ø¯Ø± ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø´ÙˆØ¯.
\end{itemize}

\subsubsection{DDQN}

\begin{itemize}
  \item \textbf{Ù…Ø¹Ù…Ø§Ø±ÛŒ:} Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ø¯Ø± DDQN Ù…Ø´Ø§Ø¨Ù‡ DQN Ø§Ø³ØªØŒ Ø¨Ø§ Ø§ÛŒÙ† ØªÙØ§ÙˆØª Ú©Ù‡ Ø¯Ø± DDQN Ø¯Ùˆ Ø´Ø¨Ú©Ù‡ Q Ø¨Ù‡â€ŒØµÙˆØ±Øª Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯: ÛŒÚ© Ø´Ø¨Ú©Ù‡ Q Ø§ØµÙ„ÛŒ Ùˆ ÛŒÚ© Ø´Ø¨Ú©Ù‡ Q Ù‡Ø¯Ù.
  \item \textbf{Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Q-Ù…Ù‚Ø§Ø¯ÛŒØ±:} Ø¯Ø± DDQNØŒ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Q-Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¨Ù‡ Ú¯ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡ Ø§Ø³Øª Ú©Ù‡ Ø§Ø² \lr{Over fitting}Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø´ÙˆØ¯. Ø§ÛŒÙ† Ú©Ø§Ø± Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø´Ø¨Ú©Ù‡ Q Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Actions Ùˆ Ø´Ø¨Ú©Ù‡ Q Ù‡Ø¯Ù Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Q-Ù…Ù‚Ø§Ø¯ÛŒØ± Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯.
  \item \textbf{Ù…Ø¹Ø§Ø¯Ù„Ù‡ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ:} Ù…Ø¹Ø§Ø¯Ù„Ù‡ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Q-Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¯Ø± DDQN Ø¨Ù‡ ØµÙˆØ±Øª Ø²ÛŒØ± Ø§Ø³Øª:
  \[
  Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma Q'(s', \arg\max_{a'} Q(s', a')) - Q(s, a) \right)
  \]
  Ú©Ù‡ Ø¯Ø± Ø¢Ù† \( Q(s', a') \) Ù…Ù‚Ø¯Ø§Ø± Q Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø´Ø¯Ù‡ ØªÙˆØ³Ø· Ø´Ø¨Ú©Ù‡ Q Ø§ØµÙ„ÛŒ Ø§Ø³Øª Ùˆ \( Q'(s', \arg\max_{a'} Q(s', a')) \) Ù…Ù‚Ø¯Ø§Ø± Q Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø´Ø¯Ù‡ ØªÙˆØ³Ø· Ø´Ø¨Ú©Ù‡ Ù‡Ø¯Ù Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ù‚Ø¯Ø§Ù… Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡ ØªÙˆØ³Ø· Ø´Ø¨Ú©Ù‡ Q Ø§ØµÙ„ÛŒ Ø§Ø³Øª.

\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=1.0\linewidth]{img/result3}
\caption{Ù…Ù‚Ø§ÛŒØ³Ù‡ DQN Ùˆ DDQN}
\label{fig:result3}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=1.0\linewidth]{img/result4}
\caption{Ù…Ù‚Ø§ÛŒØ³Ù‡ DQN Ùˆ DDQN}
\label{fig:result4}
\end{figure}

Ù‡Ù…Ø§Ù†Ø·ÙˆØ± Ú©Ù‡ Ø§Ù†ØªØ¸Ø§Ø± Ù…ÛŒâ€ŒØ±ÙØª Ù…Ø¯Ù„ DDQN Ù†ØªÛŒØ¬Ù‡ Ø¨Ù‡ØªØ± Ùˆ Ù¾Ø§ÛŒØ¯Ø§Ø±ØªØ±ÛŒ Ú©Ø³Ø¨ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª Ú©Ù‡ Ø§ÛŒÙ† Ù…Ø³Ø¦Ù„Ù‡ Ù†Ø§Ø´ÛŒ Ø§Ø² ØªØºÛŒÛŒØ± Ø±ÙˆÙ†Ø¯ Ø¢Ù…ÙˆØ²Ø´ Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯. Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ Ù…Ø¯Ù„ DDQN Ø¯Ø± ÙØ±Ø§ÛŒÙ†Ø¯ Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒ Ùˆ Ù†ØªÛŒØ¬Ù‡ Ú©Ù„ÛŒ Agent ØªØ§Ø«ÛŒØ± Ù…Ù‡Ù…ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³Øª.
\bibliographystyle{plain}
\bibliography{references}
\end{document}


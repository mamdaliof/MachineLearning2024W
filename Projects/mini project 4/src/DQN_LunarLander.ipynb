{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y swig"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VPoeimV--az",
        "outputId": "12a38a92-27c0-482d-d2bb-35517a113374"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com] [1 InRelease 14.2 kB/129 kB 11%] [Connected to cloud.r-project\r                                                                                                    \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.82)] [1 InRelease 33.0 kB/129 kB 26%] [2 InRelease\r0% [Connecting to archive.ubuntu.com (185.125.190.82)] [1 InRelease 64.8 kB/129 kB 50%] [Connecting \r                                                                                                    \rGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [973 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,125 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,591 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,994 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,664 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,409 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,258 kB]\n",
            "Fetched 13.4 MB in 3s (4,027 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 46 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 1s (1,204 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 121925 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install rarfile --quiet\n",
        "!pip install stable-baselines3 > /dev/null\n",
        "!pip install box2d-py > /dev/null\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!sudo apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "JpJvkl8Z2Dw1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B5S1SyylO8VI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b531635-3fd4-4355-9101-c3fb514e2983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Collecting gym\n",
            "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/721.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/721.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m716.8/721.7 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827620 sha256=6af18ad0351e1a03e99555ab7ebe0b25a3d58167a77a7dfadace8639b93021da\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/22/6d/3e7b32d98451b4cd9d12417052affbeeeea012955d437da1da\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.0.9 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gym-0.26.2\n",
            "Collecting pyglet\n",
            "  Downloading pyglet-2.0.15-py3-none-any.whl (884 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m884.3/884.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyglet\n",
            "Successfully installed pyglet-2.0.15\n",
            "Collecting Box2D\n",
            "  Downloading Box2D-2.3.2.tar.gz (427 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.9/427.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: Box2D\n",
            "  Building wheel for Box2D (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Box2D: filename=Box2D-2.3.2-cp310-cp310-linux_x86_64.whl size=2367283 sha256=6953a3485185c75def254a75ba7ff2632efb62a8d2ac37afe25ce8450de7c47b\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/cb/be/e663f3ce9aba6580611c0febaf7cd3cf7603f87047de2a52f9\n",
            "Successfully built Box2D\n",
            "Installing collected packages: Box2D\n",
            "Successfully installed Box2D-2.3.2\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.10/dist-packages (2.3.8)\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.10/dist-packages (0.26.2)\n",
            "\u001b[33mWARNING: gym 0.26.2 does not provide the extra 'box_2d'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[Box_2D]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[Box_2D]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[Box_2D]) (0.0.8)\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.10/dist-packages (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Collecting box2d-py==2.3.5 (from gym[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.0 (from gym[box2d])\n",
            "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting swig==4.* (from gym[box2d])\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2349114 sha256=91405076e4b46b214aaf4f1f7154197427cf69fcfd6d755557425ea1f1b8d67a\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py, pygame\n",
            "  Attempting uninstall: box2d-py\n",
            "    Found existing installation: box2d-py 2.3.8\n",
            "    Uninstalling box2d-py-2.3.8:\n",
            "      Successfully uninstalled box2d-py-2.3.8\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.6.0\n",
            "    Uninstalling pygame-2.6.0:\n",
            "      Successfully uninstalled pygame-2.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.0.9 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed box2d-py-2.3.5 pygame-2.1.0 swig-4.2.1\n"
          ]
        }
      ],
      "source": [
        "# install dependencies\n",
        "!pip3 install gym --upgrade\n",
        "!pip3 install pyglet\n",
        "!pip3 install Box2D\n",
        "!pip3 install box2d-py\n",
        "!pip3 install gym[Box_2D]\n",
        "!pip3 install gym[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --force-reinstall box2d-py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4h-16m1GOeb",
        "outputId": "f0ab2804-c526-4f41-9432-6e7ff52d1864"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting box2d-py\n",
            "  Using cached box2d_py-2.3.8-cp310-cp310-linux_x86_64.whl\n",
            "Installing collected packages: box2d-py\n",
            "  Attempting uninstall: box2d-py\n",
            "    Found existing installation: box2d-py 2.3.5\n",
            "    Uninstalling box2d-py-2.3.5:\n",
            "      Successfully uninstalled box2d-py-2.3.5\n",
            "Successfully installed box2d-py-2.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5j3em8QPiXT",
        "outputId": "1bc5d281-a0bb-40b2-c084-5d42736e8f87",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKsgfLec0msE"
      },
      "source": [
        "The input state of the Lunar Lander consists of following components:\n",
        "\n",
        "  1. Horizontal Position\n",
        "  2. Vertical Position\n",
        "  3. Horizontal Velocity\n",
        "  4. Vertical Velocity\n",
        "  5. Angle\n",
        "  6. Angular Velocity\n",
        "  7. Left Leg Contact\n",
        "  8. Right Leg Contact\n",
        "\n",
        "The actions of the agents are:\n",
        "  1. Do Nothing\n",
        "  2. Fire Main Engine\n",
        "  3. Fire Left Engine\n",
        "  4. Fire Right Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For solving error, RUN:\n",
        "\n",
        "```\n",
        "!pip install --force-reinstall box2d-py\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "pJvkfexmr_re"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AMgs5pl6Psif",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca0b1a26-17c1-4bae-f254-f3ada5ced57f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# enviroment\n",
        "import gym\n",
        "env = gym.make('LunarLander-v2')\n",
        "#TODO: find observation size: 8\n",
        "state_size = env.observation_space.shape[0]\n",
        "#TODO: find action size: 4: 0- Do nothing 1- Fire left engine 2- Fire down engine 3- Fire right engine\n",
        "action_size = env.action_space.n\n",
        "state_size, action_size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfJHEJV0HueU",
        "outputId": "d8554cb7-c219-4167-e600-10b2470eeb8e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 46 not upgraded.\n",
            "Need to get 7,813 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.10 [28.5 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.10 [863 kB]\n",
            "Fetched 7,813 kB in 0s (19.0 MB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 122678 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.10_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.10_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For solving error, RUN:\n",
        "\n",
        "```\n",
        "!apt-get install -y xvfb\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "hdA_4tn9sWto"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jPSFgqEE0msI"
      },
      "outputs": [],
      "source": [
        "# VIDEO\n",
        "import io\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import base64\n",
        "import stable_baselines3\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.results_plotter import ts2xy, load_results\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.wrappers import RecordVideo\n",
        "from gym.wrappers.monitoring import video_recorder\n",
        "from IPython.display import HTML\n",
        "from IPython import display\n",
        "import glob\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment\n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else:\n",
        "    print(\"Could not find video\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-cVE_4EtPyNz"
      },
      "outputs": [],
      "source": [
        "# experience replay\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
        "\n",
        "\n",
        "class ExperienceReplay():\n",
        "    def __init__(self, capacity) -> None:\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def store_trans(self, s, a, sp, r, done):\n",
        "        # TODO: store new transition in memory\n",
        "        transition = Transition(s, a, sp, r, done)\n",
        "        self.memory.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # TODO: take RANDOM sample from memory\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pUgDbqH4NWYZ"
      },
      "outputs": [],
      "source": [
        "# DQN\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DeepQNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size) -> None:\n",
        "        super(DeepQNetwork, self).__init__()\n",
        "        # TODO: define the architecture\n",
        "        # NOTE: input=observation/state, output=action\n",
        "        net_list = nn.ModuleList([\n",
        "            torch.nn.Linear(state_size, 512),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.LayerNorm(512),\n",
        "            torch.nn.Dropout(0.1),\n",
        "            torch.nn.Linear(512, 512),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.LayerNorm(512),\n",
        "            torch.nn.Dropout(0.1),\n",
        "            torch.nn.Linear(512, 512),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(512, action_size)\n",
        "        ])\n",
        "        self.net = torch.nn.Sequential(*net_list).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: forward propagation\n",
        "        # NOTE: use ReLu for activation function in all layers\n",
        "        # NOTE: last layer has no activation function (predict action)\n",
        "        # ReLU is created in init, no need here\n",
        "        x.to(device)\n",
        "        x = self.net(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgMI1JCLMuaf"
      },
      "source": [
        "## DQN"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MENn1lGNaMKt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mxLkRp2_P05m"
      },
      "outputs": [],
      "source": [
        "# DQN agent\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class DQNAgent():\n",
        "    # NOTE: DON'T change initial values\n",
        "    def __init__(self, state_size, action_size, batch_size,\n",
        "                 gamma=0.99, buffer_size=25000, alpha=1e-4):\n",
        "        # network parameter\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # hyperparameters\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # experience replay\n",
        "        self.experience_replay = ExperienceReplay(buffer_size)\n",
        "\n",
        "        # network\n",
        "        self.value_net = DeepQNetwork(state_size, action_size).to(device)\n",
        "\n",
        "        # optimizer\n",
        "        # TODO: create adam for optimizing network's parameter (learning rate=alpha)\n",
        "        self.optimizer = optim.Adam(self.value_net.parameters(), lr=alpha)\n",
        "\n",
        "    def take_action(self, state, eps=0.0):\n",
        "        # TODO: take action using e-greedy policy\n",
        "        # NOTE: takes action using the greedy policy with a probability of 1−𝜖 and a random action with a probability of 𝜖\n",
        "        # NOTE:\n",
        "        self.value_net.eval()\n",
        "        if len(state) != 8 :\n",
        "            state = state[0]\n",
        "        rand_eps = random.random()\n",
        "        if rand_eps > eps:\n",
        "            with torch.no_grad():\n",
        "                # print(state)\n",
        "                return torch.argmax(self.value_net(torch.tensor(state).to(device))).detach().cpu().numpy()\n",
        "        else:\n",
        "            return np.random.randint(0, self.action_size)\n",
        "\n",
        "    def update_params(self):\n",
        "        if len(self.experience_replay) < self.batch_size:\n",
        "            return\n",
        "        # transition batch\n",
        "        batch = Transition(*zip(*self.experience_replay.sample(self.batch_size)))\n",
        "\n",
        "        temp = []\n",
        "        for indx in range(len(batch.state)):\n",
        "            if len(batch.state[indx]) != 8:\n",
        "                temp.append(batch.state[indx][0])\n",
        "            else:\n",
        "                temp.append(batch.state[indx])\n",
        "        state_batch = torch.from_numpy(np.vstack(temp)).float().to(device)  # [8, 8]\n",
        "        action_batch = torch.tensor(np.vstack(batch.action)).long().to(device) # [8, 1]\n",
        "        next_state_batch = torch.from_numpy(np.vstack(batch.next_state)).float().to(device) # [8, 8]\n",
        "        reward_batch = torch.tensor(np.vstack(batch.reward)).float().to(device) # [8, 1]\n",
        "        done_batch = torch.tensor(np.vstack(batch.done)).to(device)\n",
        "\n",
        "        # calculate loss w.r.t DQN algorithm\n",
        "        self.value_net.train()\n",
        "        # STEP1\n",
        "        q_expected = self.value_net(state_batch).gather(1, action_batch)\n",
        "        # TODO: compute the expected Q values [y]\n",
        "        # STEP2\n",
        "        # TODO: compute Q values [Q(s_t, a)]\n",
        "        q_targets_next = self.value_net(next_state_batch).detach().max(1)[0].unsqueeze(1)\n",
        "        q_targets = reward_batch + (self.gamma * q_targets_next * (1 - done_batch*1))\n",
        "        # STEP3\n",
        "        # TODO: compute mse loss\n",
        "        loss = nn.functional.mse_loss(q_expected, q_targets)\n",
        "        # TODO: optimize the model\n",
        "        # NOTE: DON'T forget to set the gradients to zeros\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def save(self, fname):\n",
        "        # TODO: save checkpoint\n",
        "        torch.save(self.value_net, fname)\n",
        "\n",
        "    def load(self, fname, device):\n",
        "        # TODO: load checkpoint\n",
        "        self.value_net = torch.load(fname, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sqAOFUBb90wh"
      },
      "outputs": [],
      "source": [
        "# NOTE: DON'T change values\n",
        "n_episodes = 250\n",
        "eps = 1.0\n",
        "eps_decay_rate = 0.97\n",
        "eps_end = 0.01\n",
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ScM-yfC1P3md",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d73fabe5-5084-47a2-a5c7-2c1c0e0c64d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py:604: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 1\tAverage Reward: -121.56\tEpsilon: 0.97\rEpisode 2\tAverage Reward: -229.33\tEpsilon: 0.94\rEpisode 3\tAverage Reward: -184.11\tEpsilon: 0.91\rEpisode 4\tAverage Reward: -150.51\tEpsilon: 0.89"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py:604: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 5\tAverage Reward: -166.28\tEpsilon: 0.86"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py:604: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py:604: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py:604: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 6\tAverage Reward: -163.06\tEpsilon: 0.83\rEpisode 7\tAverage Reward: -150.45\tEpsilon: 0.81"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py:604: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 8\tAverage Reward: -146.66\tEpsilon: 0.78"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py:604: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py:604: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 9\tAverage Reward: -155.82\tEpsilon: 0.76"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 10\tAverage Reward: -159.87\tEpsilon: 0.74"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py:604: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py:604: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 11\tAverage Reward: -153.37\tEpsilon: 0.72"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py:604: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 12\tAverage Reward: -149.39\tEpsilon: 0.69"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py:604: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py:604: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 13\tAverage Reward: -139.89\tEpsilon: 0.67"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py:604: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 15\tAverage Reward: -133.91\tEpsilon: 0.63"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py:604: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py:604: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 16\tAverage Reward: -132.76\tEpsilon: 0.61"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py:604: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py:604: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py:604: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 17\tAverage Reward: -124.64\tEpsilon: 0.60"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py:604: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 19\tAverage Reward: -121.72\tEpsilon: 0.56"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py:604: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py:604: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py:604: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py:604: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpisode 20\tAverage Reward: -119.44\tEpsilon: 0.54"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/lunar_lander.py:604: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-519d0a5f8013>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_trans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# TODO: update agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;31m# TODO: update current state and episode cummulative rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# print(\"next\" ,next_state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-eabf8a38d685>\u001b[0m in \u001b[0;36mupdate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# STEP1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mq_expected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;31m# TODO: compute the expected Q values [y]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# STEP2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-f03e5c0bad66>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# ReLU is created in init, no need here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    202\u001b[0m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2571\u001b[0m             \u001b[0mlayer_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2572\u001b[0m         )\n\u001b[0;32m-> 2573\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# training phase\n",
        "\n",
        "# TODO: create agent\n",
        "agent = DQNAgent(state_size, action_size, batch_size=BATCH_SIZE)\n",
        "\n",
        "crs = np.zeros(n_episodes) # cummulative rewards\n",
        "crs_recent = deque(maxlen=25) # recent cummulative rewards\n",
        "\n",
        "# training loop\n",
        "for i_episode in range(1, n_episodes+1):\n",
        "    # TODO: initialize the environment and state\n",
        "    if i_episode % 50 == 0:\n",
        "        env = RecordVideo(gym.make(\"LunarLander-v2\"), f\"./DQN/batch{BATCH_SIZE}/eps{i_episode}\")\n",
        "    else:\n",
        "        env = gym.make(\"LunarLander-v2\")\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    cr = 0 # episode cummulative rewards\n",
        "    while not done:\n",
        "        env.render()\n",
        "        # TODO: select and perform an action\n",
        "        action = agent.take_action(state, eps)\n",
        "#         print(action)\n",
        "        # Modify the unpacking to handle the extra value if present\n",
        "        result = env.step(action)\n",
        "        if len(result) == 5:\n",
        "            next_state, reward, done, truncated, info = result\n",
        "        else:\n",
        "            next_state, reward, done, info = result\n",
        "        # TODO: store transition in experience replay\n",
        "        agent.experience_replay.store_trans(state, action, next_state, reward, done)\n",
        "        # TODO: update agent\n",
        "        agent.update_params()\n",
        "        # TODO: update current state and episode cummulative rewards\n",
        "        # print(\"next\" ,next_state)\n",
        "        state = next_state\n",
        "        cr += reward\n",
        "\n",
        "    # TODO: decay epsilon\n",
        "    eps = eps * eps_decay_rate\n",
        "    eps = max(eps, eps_end)\n",
        "    # TODO: update current cummulative rewards and recent cummulative rewards\n",
        "    crs[i_episode - 1] = cr\n",
        "    crs_recent.append(cr)\n",
        "    # TODO: save agent every 50 episodes\n",
        "    if i_episode % 50 == 0:\n",
        "        agent.save(f\"q_net_batch{BATCH_SIZE}_eps{i_episode}.pt\")\n",
        "\n",
        "    # print logs\n",
        "    print('\\rEpisode {}\\tAverage Reward: {:.2f}\\tEpsilon: {:.2f}'.format(i_episode, np.mean(crs_recent), eps), end=\"\")\n",
        "    if i_episode % 25 == 0:\n",
        "        print('\\rEpisode {}\\tAverage Reward: {:.2f}\\tEpsilon: {:.2f}'.format(i_episode, np.mean(crs_recent), eps))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crs32 = crs"
      ],
      "metadata": {
        "id": "rCJcz-bmll9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJbOnslW0msT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure()\n",
        "plt.plot(np.arange(len(crs)), crs)\n",
        "plt.ylabel('Reward')\n",
        "plt.xlabel('Episode')\n",
        "plt.title(f\"DQN_batch{BATCH_SIZE}\")\n",
        "plt.savefig(f\"DQN_batch{BATCH_SIZE}.pdf\")\n",
        "plt.show()\n",
        "! zip -r DQN.zip DQN/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch 64"
      ],
      "metadata": {
        "id": "aaoFs4pCCfab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: DON'T change values\n",
        "n_episodes = 250\n",
        "eps = 1.0\n",
        "eps_decay_rate = 0.97\n",
        "eps_end = 0.01\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "jOmmJgg_CkSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training phase\n",
        "\n",
        "# TODO: create agent\n",
        "agent = DQNAgent(state_size, action_size, batch_size=BATCH_SIZE)\n",
        "\n",
        "crs = np.zeros(n_episodes) # cummulative rewards\n",
        "crs_recent = deque(maxlen=25) # recent cummulative rewards\n",
        "\n",
        "# training loop\n",
        "for i_episode in range(1, n_episodes+1):\n",
        "    # TODO: initialize the environment and state\n",
        "    if i_episode % 50 == 0:\n",
        "        env = RecordVideo(gym.make(\"LunarLander-v2\"), f\"./DQN/batch{BATCH_SIZE}/eps{i_episode}\")\n",
        "    else:\n",
        "        env = gym.make(\"LunarLander-v2\")\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    cr = 0 # episode cummulative rewards\n",
        "    while not done:\n",
        "        env.render()\n",
        "        # TODO: select and perform an action\n",
        "        action = agent.take_action(state, eps)\n",
        "        next_state, reward, done, _,info = env.step(action)\n",
        "        # TODO: store transition in experience replay\n",
        "        agent.experience_replay.store_trans(state, action, next_state, reward, done)\n",
        "        # TODO: update agent\n",
        "        agent.update_params()\n",
        "        # TODO: update current state and episode cummulative rewards\n",
        "        state = next_state\n",
        "        cr += reward\n",
        "\n",
        "    # TODO: decay epsilon\n",
        "    eps = eps * eps_decay_rate\n",
        "    eps = max(eps, eps_end)\n",
        "    # TODO: update current cummulative rewards and recent cummulative rewards\n",
        "    crs[i_episode - 1] = cr\n",
        "    crs_recent.append(cr)\n",
        "    # TODO: save agent every 50 episodes\n",
        "    if i_episode % 50 == 0:\n",
        "        agent.save(f\"q_net_batch{BATCH_SIZE}_eps{i_episode}.pt\")\n",
        "\n",
        "    # print logs\n",
        "    print('\\rEpisode {}\\tAverage Reward: {:.2f}\\tEpsilon: {:.2f}'.format(i_episode, np.mean(crs_recent), eps), end=\"\")\n",
        "    if i_episode % 25 == 0:\n",
        "        print('\\rEpisode {}\\tAverage Reward: {:.2f}\\tEpsilon: {:.2f}'.format(i_episode, np.mean(crs_recent), eps))\n",
        "crs64 = crs\n",
        "fig = plt.figure()\n",
        "plt.plot(np.arange(len(crs)), crs)\n",
        "plt.ylabel('Reward')\n",
        "plt.xlabel('Episode')\n",
        "plt.title(f\"DQN_batch{BATCH_SIZE}\")\n",
        "plt.savefig(f\"DQN_batch{BATCH_SIZE}.pdf\")\n",
        "plt.show()\n",
        "! zip -r DQN2.zip DQN/"
      ],
      "metadata": {
        "id": "E-lLpFx0Cm9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BATCH 128"
      ],
      "metadata": {
        "id": "4b1Jry91HTNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: DON'T change values\n",
        "n_episodes = 250\n",
        "eps = 1.0\n",
        "eps_decay_rate = 0.97\n",
        "eps_end = 0.01\n",
        "BATCH_SIZE = 128"
      ],
      "metadata": {
        "id": "2ShseY26HVJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training phase\n",
        "\n",
        "# TODO: create agent\n",
        "agent = DQNAgent(state_size, action_size, batch_size=BATCH_SIZE)\n",
        "\n",
        "crs = np.zeros(n_episodes) # cummulative rewards\n",
        "crs_recent = deque(maxlen=25) # recent cummulative rewards\n",
        "\n",
        "# training loop\n",
        "for i_episode in range(1, n_episodes+1):\n",
        "    # TODO: initialize the environment and state\n",
        "    if i_episode % 50 == 0:\n",
        "        env = RecordVideo(gym.make(\"LunarLander-v2\"), f\"./DQN/batch{BATCH_SIZE}/eps{i_episode}\")\n",
        "    else:\n",
        "        env = gym.make(\"LunarLander-v2\")\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    cr = 0 # episode cummulative rewards\n",
        "    while not done:\n",
        "        env.render()\n",
        "        # TODO: select and perform an action\n",
        "        action = agent.take_action(state, eps)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        # TODO: store transition in experience replay\n",
        "        agent.experience_replay.store_trans(state, action, next_state, reward, done)\n",
        "        # TODO: update agent\n",
        "        agent.update_params()\n",
        "        # TODO: update current state and episode cummulative rewards\n",
        "        state = next_state\n",
        "        cr += reward\n",
        "\n",
        "    # TODO: decay epsilon\n",
        "    eps = eps * eps_decay_rate\n",
        "    eps = max(eps, eps_end)\n",
        "    # TODO: update current cummulative rewards and recent cummulative rewards\n",
        "    crs[i_episode - 1] = cr\n",
        "    crs_recent.append(cr)\n",
        "    # TODO: save agent every 50 episodes\n",
        "    if i_episode % 50 == 0:\n",
        "        agent.save(f\"q_net_batch{BATCH_SIZE}_eps{i_episode}.pt\")\n",
        "\n",
        "    # print logs\n",
        "    print('\\rEpisode {}\\tAverage Reward: {:.2f}\\tEpsilon: {:.2f}'.format(i_episode, np.mean(crs_recent), eps), end=\"\")\n",
        "    if i_episode % 25 == 0:\n",
        "        print('\\rEpisode {}\\tAverage Reward: {:.2f}\\tEpsilon: {:.2f}'.format(i_episode, np.mean(crs_recent), eps))\n",
        "crs128 = crs\n",
        "fig = plt.figure()\n",
        "plt.plot(np.arange(len(crs)), crs)\n",
        "plt.ylabel('Reward')\n",
        "plt.xlabel('Episode')\n",
        "plt.title(f\"DQN_batch{BATCH_SIZE}\")\n",
        "plt.savefig(f\"DQN_batch{BATCH_SIZE}.pdf\")\n",
        "plt.show()\n",
        "! zip -r DQN3.zip DQN/"
      ],
      "metadata": {
        "id": "ZkxBaA5wHWgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lXfgBFOMx0G"
      },
      "source": [
        "## DDQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifTBKlEigZEu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "\n",
        "class DDQNAgent():\n",
        "    # NOTE: DON'T change initial values\n",
        "    def __init__(self, state_size, action_size, batch_size,\n",
        "                 gamma=0.99, buffer_size=25000, alpha=1e-4):\n",
        "        # network parameter\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # hyperparameters\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # experience replay\n",
        "        self.experience_buffer = ExperienceReplay(buffer_size)\n",
        "\n",
        "        # networks\n",
        "        self.value_net = DeepQNetwork(state_size, action_size).to(device)\n",
        "        self.target_value_net = DeepQNetwork(state_size, action_size).to(device)\n",
        "        self.update_target_network()\n",
        "\n",
        "        # optimizer\n",
        "        # TODO: create adam for optimizing network's parameter (learning rate=alpha)\n",
        "        # NOTE: target network parameters DOSEN'T update with optimizer\n",
        "        self.optimizer = optim.Adam(self.value_net.parameters(), lr=alpha)\n",
        "\n",
        "    def take_action(self, state, eps=0.0):\n",
        "        # TODO: take action using e-greedy policy\n",
        "        # NOTE: takes action using the greedy policy with a probability of 1−𝜖 and a random action with a probability of 𝜖\n",
        "        # NOTE:\n",
        "        self.value_net.eval()\n",
        "        rand_eps = random.random()\n",
        "        if rand_eps > eps:\n",
        "            with torch.no_grad():\n",
        "                return torch.argmax(self.value_net(torch.tensor(state).to(device))).detach().cpu().numpy()\n",
        "        else:\n",
        "            return np.random.randint(0, self.action_size)\n",
        "\n",
        "    def update_params(self):\n",
        "        if len(self.experience_buffer) < self.batch_size:\n",
        "            return\n",
        "        # transition batch\n",
        "        batch = Transition(*zip(*self.experience_buffer.sample(self.batch_size)))\n",
        "\n",
        "        state_batch = torch.from_numpy(np.vstack(batch.state)).float().to(device)\n",
        "        action_batch = torch.tensor(np.vstack(batch.action)).long().to(device)\n",
        "        next_state_batch = torch.from_numpy(np.vstack(batch.next_state)).float().to(device)\n",
        "        reward_batch = torch.tensor(np.vstack(batch.reward)).float().to(device)\n",
        "        done_batch = torch.tensor(np.vstack(batch.done)).to(device)\n",
        "\n",
        "        # calculate loss w.r.t DQN algorithm\n",
        "        self.value_net.train()\n",
        "        # STEP1\n",
        "        q_targets_next = self.target_value_net(next_state_batch).detach().max(1)[0].unsqueeze(1)\n",
        "        # STEP2\n",
        "        # TODO: compute Q values [Q(s_t, a)]\n",
        "        q_targets = reward_batch + self.gamma * q_targets_next * (1 - done_batch*1)\n",
        "        q_expected = self.value_net(state_batch).gather(1, action_batch)\n",
        "        # STEP3\n",
        "        # TODO: compute mse loss\n",
        "        loss = nn.functional.mse_loss(q_expected, q_targets)\n",
        "        # TODO: optimize the model\n",
        "        # NOTE: DON'T forget to set the gradients to zeros\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        # TODO: copy main network parameters to target network parameters\n",
        "        self.target_value_net = copy.deepcopy(self.value_net)\n",
        "\n",
        "    def save(self, fname):\n",
        "        # TODO: save checkpoint\n",
        "        torch.save(self.value_net, fname)\n",
        "\n",
        "    def load(self, fname, device):\n",
        "        # TODO: load checkpoint\n",
        "        self.value_net = torch.load(fname, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: DON'T change values\n",
        "n_episodes = 250\n",
        "eps = 1.0\n",
        "eps_decay_rate = 0.97\n",
        "eps_end = 0.01\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "5Lf8CtwvS1Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnSaedlKjnQe"
      },
      "outputs": [],
      "source": [
        "# training phase\n",
        "\n",
        "# TODO: create agent\n",
        "agent = DDQNAgent(state_size, action_size, batch_size=BATCH_SIZE)\n",
        "\n",
        "crs = np.zeros(n_episodes) # cummulative rewards\n",
        "crs_recent = deque(maxlen=25) # recent cummulative rewards\n",
        "\n",
        "# training loop\n",
        "for i_episode in range(1, n_episodes+1):\n",
        "    # TODO: initialize the environment and state\n",
        "    if i_episode % 50 == 0:\n",
        "        env = RecordVideo(gym.make(\"LunarLander-v2\"), f\"./DDQN/batch{BATCH_SIZE}/eps{i_episode}\")\n",
        "    else:\n",
        "        env = gym.make(\"LunarLander-v2\")\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    cr = 0 # episode cummulative rewards\n",
        "    action_count = 0\n",
        "    while not done:\n",
        "        env.render()\n",
        "        # TODO: select and perform an action\n",
        "        action = agent.take_action(state, eps)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        # TODO: store transition in experience replay\n",
        "        agent.experience_buffer.store_trans(state, action, next_state, reward, done)\n",
        "        # TODO: update agent\n",
        "        agent.update_params()\n",
        "        # TODO: update current state and episode cummulative rewards\n",
        "        state = next_state\n",
        "        cr += reward\n",
        "        action_count += 1\n",
        "        if action_count % 5 == 0:\n",
        "            agent.update_target_network()\n",
        "\n",
        "    # TODO: decay epsilon\n",
        "    eps = eps * eps_decay_rate\n",
        "    eps = max(eps, eps_end)\n",
        "\n",
        "    # TODO: update current cummulative rewards and recent cummulative rewards\n",
        "    crs[i_episode - 1] = cr\n",
        "    crs_recent.append(cr)\n",
        "\n",
        "    # TODO: save agent every 50 episodes\n",
        "    if i_episode % 50 == 0:\n",
        "        agent.save(f\"q_net_batch{BATCH_SIZE}_eps{i_episode}.pt\")\n",
        "\n",
        "    # print logs\n",
        "    print('\\rEpisode {}\\tAverage Reward: {:.2f}\\tEpsilon: {:.2f}'.format(i_episode, np.mean(crs_recent), eps), end=\"\")\n",
        "    if i_episode % 25 == 0:\n",
        "        print('\\rEpisode {}\\tAverage Reward: {:.2f}\\tEpsilon: {:.2f}'.format(i_episode, np.mean(crs_recent), eps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnoK-4dpooFG"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(np.arange(len(crs)), crs)\n",
        "plt.ylabel('Reward')\n",
        "plt.xlabel('Episode')\n",
        "plt.title(f\"DDQN_batch{BATCH_SIZE}\")\n",
        "plt.savefig(f\"DDQN_batch{BATCH_SIZE}.pdf\")\n",
        "plt.show()\n",
        "! zip -r DDQN3.zip DDQN/"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-JrH8X9NY5DU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}